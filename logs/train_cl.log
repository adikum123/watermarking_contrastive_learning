Loading precomputed ljspeech datasets
2025-09-25 14:15:09,511 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-09-25 14:15:09,512 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-09-25 14:15:09,512 [INFO] Device: cuda
2025-09-25 14:15:11,667 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 15
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.001,
        "lambda_e": 1.0,
        "lambda_m": 0.05,
        "lambda_a": 0.01,
        "lambda_cl": 0.1,
        "batch_size": 2,
        "step_size": 2,
        "gamma": 0.9,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 16,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 20,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-09-25 14:15:11,667 [INFO] Epoch: 1
/home/sagemaker-user/watermarking_contrastive_learning/loss/contrastive_loss.py:152: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4421.)
  torch.matmul(anchors, features.T) / self.temperature
threshold_factor je  0.5
Traceback (most recent call last):
  File "/home/sagemaker-user/watermarking_contrastive_learning/train/contrastive_pretrain_decoder.py", line 122, in <module>
    cl_loss = loss(
              ^^^^^
  File "/home/sagemaker-user/.cache/pypoetry/virtualenvs/watermarking-contrastive-learning-WvXdt30y-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sagemaker-user/.cache/pypoetry/virtualenvs/watermarking-contrastive-learning-WvXdt30y-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sagemaker-user/watermarking_contrastive_learning/loss/contrastive_loss.py", line 124, in forward
    return self.info_nce_loss(f1, f2, positive_extra)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sagemaker-user/watermarking_contrastive_learning/loss/contrastive_loss.py", line 152, in info_nce_loss
    torch.matmul(anchors, features.T) / self.temperature
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: The size of tensor a (2) must match the size of tensor b (110250) at non-singleton dimension 0
