nohup: ignoring input
Loading precomputed ljspeech datasets
2025-10-03 22:42:34,990 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-10-03 22:42:34,991 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-10-03 22:42:34,991 [INFO] Device: cuda
2025-10-03 22:42:38,304 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 10
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.0003,
        "batch_size": 2,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 32,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10,
        "train_size": 31000
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 5,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-05 12:58:43,483 [INFO] Device: cuda
2025-10-05 12:59:44,410 [INFO] Device: cuda
2025-10-05 13:01:23,712 [INFO] Device: cuda
2025-10-05 13:01:43,426 [INFO] Device: cuda
2025-10-05 13:01:52,735 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 10
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.0003,
        "batch_size": 2,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 32,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10,
        "train_size": 31000
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 5,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-05 13:02:33,464 [INFO] Device: cuda
2025-10-05 13:02:39,003 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 10
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.0003,
        "batch_size": 2,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 32,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10,
        "train_size": 31000
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 5,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-05 13:04:41,356 [INFO] Device: cuda
2025-10-05 13:04:51,421 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 10
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.0003,
        "batch_size": 2,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 32,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10,
        "train_size": 31000
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 5,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-05 13:06:10,646 [INFO] Device: cuda
2025-10-05 13:06:13,387 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 10
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.0003,
        "batch_size": 2,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 32,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10,
        "train_size": 31000
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 5,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-05 13:07:17,572 [INFO] Device: cuda
2025-10-05 13:07:26,951 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 10
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.0003,
        "batch_size": 2,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 32,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10,
        "train_size": 31000
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 5,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-05 13:11:15,550 [INFO] Device: cuda
2025-10-05 13:11:18,320 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 10
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.0003,
        "batch_size": 2,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 32,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10,
        "train_size": 31000
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 5,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-05 13:11:49,808 [INFO] Device: cuda
2025-10-05 13:12:01,845 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 10
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.0003,
        "batch_size": 2,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 32,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10,
        "train_size": 31000
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 5,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-05 13:12:09,158 [INFO] Processed 1 batches
2025-10-05 13:12:09,158 [INFO] {
    "loss": 3.5856655,
    "pesq": 1.06576,
    "avg_acc_identity": 0.6,
    "avg_acc_distorted": 0.6
}
2025-10-05 13:13:32,229 [INFO] Device: cuda
2025-10-05 13:13:34,966 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 10
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.0003,
        "batch_size": 2,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 32,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10,
        "train_size": 31000
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 5,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-05 13:13:47,257 [INFO] Processed 1 batches
2025-10-05 13:13:47,257 [INFO] {
    "loss": 3.5224882,
    "pesq": 1.06576,
    "avg_acc_identity": 0.6,
    "avg_acc_distorted": 0.6
}
2025-10-05 13:21:59,183 [INFO] Device: cuda
2025-10-05 13:22:01,962 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 10
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.0003,
        "batch_size": 2,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 32,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10,
        "train_size": 31000
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 5,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-05 13:22:12,309 [INFO] Processed 1 batches
2025-10-05 13:22:12,310 [INFO] {
    "loss": 4.1941397,
    "pesq": 1.12941,
    "avg_acc_identity": 0.4,
    "avg_acc_distorted": 0.4
}
