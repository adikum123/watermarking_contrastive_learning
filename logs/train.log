2025-09-25 14:08:02,523 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-09-25 14:08:02,524 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-09-25 14:08:02,525 [INFO] Device: cuda
2025-09-25 14:11:05,114 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-09-25 14:11:05,114 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-09-25 14:11:05,114 [INFO] Device: cuda
2025-09-25 14:12:51,022 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-09-25 14:12:51,023 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-09-25 14:12:51,023 [INFO] Device: cuda
2025-09-25 14:13:20,167 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-09-25 14:13:20,167 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-09-25 14:13:20,167 [INFO] Device: cuda
2025-09-25 14:13:22,339 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 15
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.001,
        "lambda_e": 1.0,
        "lambda_m": 0.05,
        "lambda_a": 0.01,
        "lambda_cl": 0.1,
        "batch_size": 2,
        "step_size": 2,
        "gamma": 0.9,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 16,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 20,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-09-25 14:13:22,339 [INFO] Epoch: 1
2025-09-25 14:14:17,338 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-09-25 14:14:17,338 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-09-25 14:14:17,338 [INFO] Device: cuda
2025-09-25 14:14:19,473 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 15
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.001,
        "lambda_e": 1.0,
        "lambda_m": 0.05,
        "lambda_a": 0.01,
        "lambda_cl": 0.1,
        "batch_size": 2,
        "step_size": 2,
        "gamma": 0.9,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 16,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 20,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-09-25 14:14:19,473 [INFO] Epoch: 1
2025-09-25 14:14:40,165 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-09-25 14:14:40,166 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-09-25 14:14:40,166 [INFO] Device: cuda
2025-09-25 14:14:42,405 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 15
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.001,
        "lambda_e": 1.0,
        "lambda_m": 0.05,
        "lambda_a": 0.01,
        "lambda_cl": 0.1,
        "batch_size": 2,
        "step_size": 2,
        "gamma": 0.9,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 16,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 20,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-09-25 14:14:42,405 [INFO] Epoch: 1
2025-09-25 14:15:09,511 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-09-25 14:15:09,512 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-09-25 14:15:09,512 [INFO] Device: cuda
2025-09-25 14:15:11,667 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 15
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.001,
        "lambda_e": 1.0,
        "lambda_m": 0.05,
        "lambda_a": 0.01,
        "lambda_cl": 0.1,
        "batch_size": 2,
        "step_size": 2,
        "gamma": 0.9,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 16,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 20,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-09-25 14:15:11,667 [INFO] Epoch: 1
2025-10-01 19:57:52,827 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-10-01 19:57:52,829 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-10-01 19:57:52,829 [INFO] Device: cuda
2025-10-01 19:58:41,935 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-10-01 19:58:41,936 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-10-01 19:58:41,936 [INFO] Device: cuda
2025-10-01 20:00:07,079 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-10-01 20:00:07,080 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-10-01 20:00:07,080 [INFO] Device: cuda
2025-10-01 20:00:10,363 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 15
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.001,
        "batch_size": 2,
        "step_size": 2,
        "gamma": 0.9,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 16,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 20,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-01 20:00:10,363 [INFO] Epoch: 1
2025-10-01 20:00:10,439 [WARNING] Pyrubberband failed: Failed to execute rubberband. Please verify that rubberband-cli is installed.. Falling back to librosa time_stretch.
2025-10-01 20:00:19,176 [WARNING] Pyrubberband failed: Failed to execute rubberband. Please verify that rubberband-cli is installed.. Falling back to librosa time_stretch.
2025-10-01 20:00:20,002 [WARNING] Pyrubberband failed: Failed to execute rubberband. Please verify that rubberband-cli is installed.. Falling back to librosa time_stretch.
2025-10-01 20:01:43,953 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-10-01 20:01:43,953 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-10-01 20:01:43,954 [INFO] Device: cuda
2025-10-01 20:01:46,217 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 15
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.001,
        "batch_size": 2,
        "step_size": 2,
        "gamma": 0.9,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 16,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 20,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-01 20:01:46,218 [INFO] Epoch: 1
2025-10-01 20:02:42,832 [INFO] Replaced:34.22% of blocks.
2025-10-01 20:03:15,015 [INFO] Replaced:31.82% of blocks.
2025-10-01 20:05:53,669 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-10-01 20:05:53,670 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-10-01 20:05:53,670 [INFO] Device: cuda
2025-10-01 20:05:55,972 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 15
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.001,
        "batch_size": 2,
        "step_size": 2,
        "gamma": 0.9,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 16,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 20,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-01 20:05:55,973 [INFO] Epoch: 1
2025-10-01 20:06:17,518 [INFO] Replaced:28.84% of blocks.
2025-10-01 20:06:26,765 [INFO] Replaced:32.75% of blocks.
2025-10-01 20:09:09,091 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-10-01 20:09:09,092 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-10-01 20:09:09,092 [INFO] Device: cuda
2025-10-01 20:09:11,378 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 15
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.001,
        "batch_size": 2,
        "step_size": 2,
        "gamma": 0.9,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 16,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 20,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-01 20:09:11,379 [INFO] Epoch: 1
2025-10-01 20:10:30,710 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-10-01 20:10:30,710 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-10-01 20:10:30,710 [INFO] Device: cuda
2025-10-01 20:10:33,277 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 15
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.001,
        "batch_size": 2,
        "step_size": 2,
        "gamma": 0.9,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 16,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 20,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-01 20:10:33,278 [INFO] Epoch: 1
2025-10-01 20:11:46,224 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-10-01 20:11:46,225 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-10-01 20:11:46,225 [INFO] Device: cuda
2025-10-01 20:11:48,464 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 15
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.001,
        "batch_size": 2,
        "step_size": 2,
        "gamma": 0.9,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 16,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 20,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-01 20:11:48,465 [INFO] Epoch: 1
2025-10-01 20:12:01,766 [INFO] Replaced:39.84% of blocks.
2025-10-01 20:13:13,558 [INFO] :package: train loader ready → samples=12000, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-10-01 20:13:13,559 [INFO] :package: train loader ready → samples=500, batch_size=2, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-10-01 20:13:13,559 [INFO] Device: cuda
2025-10-01 20:13:20,210 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 15
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.001,
        "batch_size": 2,
        "step_size": 2,
        "gamma": 0.9,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 16,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 20,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-01 20:13:20,210 [INFO] Epoch: 1
2025-10-01 20:16:17,315 [INFO] :package: train loader ready → samples=12000, batch_size=4, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-10-01 20:16:17,316 [INFO] :package: train loader ready → samples=500, batch_size=4, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-10-01 20:16:17,316 [INFO] Device: cuda
2025-10-01 20:16:19,550 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 15
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.001,
        "batch_size": 4,
        "step_size": 2,
        "gamma": 0.9,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 16,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 20,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-01 20:16:19,550 [INFO] Epoch: 1
2025-10-01 20:24:28,868 [INFO] :package: train loader ready → samples=12000, batch_size=4, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-10-01 20:24:28,869 [INFO] :package: train loader ready → samples=500, batch_size=4, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-10-01 20:24:28,869 [INFO] Device: cuda
2025-10-01 20:24:31,168 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 15
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.001,
        "batch_size": 4,
        "step_size": 2,
        "gamma": 0.9,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 16,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 20,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-01 20:24:31,169 [INFO] Epoch: 1
2025-10-01 20:25:41,061 [INFO] :package: train loader ready → samples=12000, batch_size=4, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=True
2025-10-01 20:25:41,062 [INFO] :package: train loader ready → samples=500, batch_size=4, total_workers=0, workers_per_gpu=0, num_gpus=1, shuffle=False
2025-10-01 20:25:41,062 [INFO] Device: cuda
2025-10-01 20:25:43,473 [INFO] Training with params:
{
    "adv": false,
    "watermark": {
        "length": 15
    },
    "optimize": {
        "gradient_scaling": true,
        "lr": 0.001,
        "batch_size": 4,
        "step_size": 2,
        "gamma": 0.9,
        "betas": [
            0.9,
            0.98
        ],
        "eps": 1e-09,
        "weight_decay": 0.0,
        "grad_clip_thresh": 1.0,
        "grad_acc_step": 16,
        "up_step": 4000,
        "anneal_steps": [
            300000,
            400000,
            500000
        ],
        "anneal_rate": 0.3,
        "lr_disc": 2e-05,
        "alpha": 10
    },
    "contrastive": {
        "loss_type": "info_nce"
    },
    "iter": {
        "epoch": 20,
        "save_circle": 5,
        "show_circle": 100,
        "val_circle": 100
    }
}
Length of train dataset: 12000
2025-10-01 20:25:43,474 [INFO] Epoch: 1
